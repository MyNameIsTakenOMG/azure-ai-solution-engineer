# AI-SOLUTION-REVIEW

## table of contents
 - [Get started with Azure AI Services](#get-started-with-azure-ai-services)
 - [Create computer vision solutions with Azure AI Vision](#create-computer-vision-solutions-with-azure-ai-vision)
 - [Develop natural language processing solutions with Azure AI Services](#develop-natural-language-processing-solutions-with-azure-ai-services)
 - [Implement knowledge mining with Azure AI Search](#implement-knowledge-mining-with-azure-ai-search)
 - [Develop solutions with Azure AI Document Intelligence](#develop-solutions-with-azure-ai-document-intelligence)
 - [Develop Generative AI solutions with Azure OpenAI Service](#develop-generative-ai-solutions-with-azure-openai-service)
 - [Assessments](#assessments)

## Get started with Azure AI Services

 - Prepare to develop AI solutions on Azure
   - Define artificial intelligence: AI as software that exhibits one or more human-like capabilities: `visual perception`,`text analysis and conversation`,`speech`,`decision making`
   - Understand AI-related terms:
     - **data science**: Data science is a discipline that focuses on the processing and analysis of data; applying statistical techniques to uncover and visualize relationships and patterns in the data, and defining experimental `models` that help explore those patterns.
     - **machine learning**: Machine learning is a subset of data science that deals with the training and validation of `predictive models`. Typically, a data scientist prepares the data and then uses it to train a model based on an algorithm that exploits the relationships between the `features` in the data to predict values for unknown `labels`.
     - **artificial intelligence**: Artificial intelligence usually (but not always) builds on machine learning to create software that emulates one or more characteristics of human intelligence.
   - Understand considerations for AI Engineers:
     - **model training and inferencing**: Many AI systems rely on predictive models that must be `trained` using sample data. The training process analyzes the data and determines relationships between the `features` in the data and the `label`. After the model has been trained, you can submit new data that includes known `feature` values and have the model predict the most likely `label`. Using the model to make predictions is referred to as `inferencing`. Many services and frameworks require the process of training a model using exsiting data before using ti to inference new values.
     - **probability and confidence scores**: A well-trained machine learning model can be accurate, but no predictive model is infallible. In most cases, predictions have an associated confidence score that reflects the probability on which the prediction is being made. Software developers should make use of confidence score values to evaluate predictions and apply appropriate thresholds to optimize application reliability and mitigate the risk of predictions that may be made based on marginal probabilities.
     - **responsible ai and ethics**: It's important for software engineers to consider the impact of their software on users, and society in general; including ethical considerations about its use. The human-like nature of AI solutions is a significant benefit in making applications user-friendly, but it can also lead users to place a great deal of trust in the application's ability to make correct decisions. The potential for harm to individuals or groups through incorrect predictions or misuse of AI capabilities is a major concern, and software engineers building AI-enabled solutions should apply due consideration to mitigate risks and ensure fairness, reliability, and adequate protection from harm or discrimination.
   - Understand considerations for responsible AI:
     - **fairness**: AI systems should treat all people fairly. (no bias based on gender, ethnicity or etc)
     - **reliability and safety**: AI systems should perform relibly and safely. Unreliability in these kinds of system can result in substantial risk to human life. As with any software, AI-based software application development must be subjected to rigorous testing and deployment management processes to ensure that they work as expected before release.
     - **privacy and security**: AI systems should be secure and respect privacy. Personal details must be kept private.
     - **inclusiveness**: AI systems should empower everyone and engage people regardless of physical ability, gender, sexual orientation, ethnicity, or other factors.
     - **transparency**: AI systems should be understandable. Users should be made fully aware of the purpose of the system, how it works, and what limitations may be expected.
     - **accountability**: People should be accountable for AI systems. Although many AI systems seem to operate autonomously, ultimately it's the responsibility of the developers who trained and validated the models they use, and defined the logic that bases decisions on model predictions to ensure that the overall system meets responsibility requirements.
   - Understand capabilities of Azure Machine Learning: a cloud-based platform for running experiments at scale to train predictive models from data, and publish the trained models as services. Features:
     - **Automated machine learning**
     - **Azure Machine Learning designer**
     - **Data and compute management**
     - **Pipelines**
   - Understand capabilities of Azure AI Services:
     - **natural language processing**: text analysis, QnA, translation, speech...
     - **knowledge mining and document intelligence**: ai search, document intelligence, custom document intelligence, custom skills...
     - **computer vision**: image analysis, face, video analysis, OCR...
     - **decision support**: content moderation...
     - **generative ai**: azure openai, dall-e image generation
   - Understand capabilities of Azure OpenAI Service: Generative AI models depend on large language models (LLMs) based on the transformer architecture that evolved from years of machine learning progress. Generative AI models are often queried with natural language prompts, and return an impressively accurate response when prompted correctly. `Azure OpenAI Service` is an Azure AI service for deploying, utilizing, and fine-tuning models developed by OpenAI. 
   - Understand capabilities of Azure AI Search: Azure AI Search is an Applied AI Service that enables you to ingest and index data from various sources, and search the index to find, filter, and sort information extracted from the source data. In addition to basic text-based indexing, Azure AI Search enables you to define an `enrichment pipeline` that uses AI skills to enhance the index with insights derived from the source data. Not only does this AI enrichment produce a more useful search experience, the insights extracted by your enrichment pipeline can be persisted in a knowledge store for further analysis or integration into a data pipeline for a business intelligence solution.
 - Create and consume Azure AI services: Azure AI services includes a wide range of individual services across multiple categories: `language`,`speech`,`vision`,`decision`. Different services can work together as a solution for certain scenario, such as `Azure AI document intelligence`,`Azure AI immersive reader`,`Azure cognitive search`,`Azure openai`
   - Create Azure AI services resources in an Azure subscription
     - multi-service resource and single-service resource(separated endpoints)
     - Training and prediction resources: some services offer separate resources for model training and prediction.
   - Identify endpoints, keys, and locations required to consume an Azure AI services resource.
     - `endpoint`: http address through which the service can be accessed.
     - `subscription key`: a valid key that must be provided when accessing endpoints
     - `resource location`: when provisioning a resource in Azure, this resource is assigned to a location, whhich determines the Azure data certer.
   - Use a REST API and an SDK to consume Azure AI services.
 - Secure Azure AI services
   - Consider authentication for Azure AI services
     - `regenerate subscription keys` (two keys for each ai service)
     - `protect keys with azure key vault`: Azure key vault can securely store secrets(passwords or keys). Access to key vault is granted to `security principals` which are authenticated user identities using `Microsoft Entra ID`. A security principal can be assigned to an application to define a `managed identity` for the application.
     - `token-based authentication`: When using the REST interface, some AI services support (or even require) token-based authentication. In these cases, the subscription key is presented in an initial request to obtain an authentication token, which has a valid period of 10 minutes. Subsequent requests must present the token to validate that the caller has been authenticated. (when using sdk, obtaining and presenting tokens are handled by sdk)
     - `Microsoft Entra ID authentication`: can be used to grant access to specific `service principals` or `managed identities` for apps and services.
       - `authenticate using service principals`
         - create a custom subdomain
         - assign a role to a service principal for you app
       - `authenticate using managed identities`
         - `system-assigned managed identity`: one-to-one, deleted when service deleted
         - `user-assigned managed identity`: reusable identity
   - Manage network security for Azure AI services: by default, azure ai services are accessible from all networks. But we can go to `networking/firewalls and virtual networks` to enable network restrictions.
 - Monitor Azure AI services
   - Monitor Azure AI services costs: using `azure pricing calculator` to create a new `estimate` and select an azure ai service you want to use (can choose multiple services). To view costs for the ai services, go to subscription --> cost analysis tab and view only costs for certain service by adding filters.
   - Create alerts and view metrics for Azure AI services
     - create alerts: create `alert rules` to configure notifications and alerts for your resources based on events or metric thresholds. `scope`: the resource to monitor; `condition`: on which the alert is triggered(`signal type`: `activity log` or `metric`); `optional actions`: sending emails or running azure logic aap to address the issue; `alert rule details`: such as the name of the alert rule and resource group where it should be defined.
     - view metrics: can add multiple metrics to a chart and choose appropriate aggregations and chart types.
     - add metrics to a dashboard
   - Manage Azure AI services diagnostic logging
     - create resources for diagnostic log storage: you can use Azure Event Hub as a destination so that then forward the data to a custom telemetry solution or 3rd party solution. But in most cases, you can use `Azure log analytics`, or `Azure storage`. **note**: create these resources before configuring diagnostic logging for your services
     - Configure diagnostic settings: go to `Diagnotic settings`, and specify the `name`, `catogories` of log event data to be captured, `details` of the destinations to store the data.
     - View log data in Azure Log Analytics
 - Deploy Azure AI services in containers
   - Create containers for reuse: pull container images from a registry and deploy it to a container host.
   - Deploy to a container and secure a container: download a contain image for specific azure ai service api and deploy it to ACI, or AKS or a local docker server. the usage metrics will be sent to azure ai services resource to calculate the billing.
     - configuration: `apikey`, `billing`,`eula`
     - containers:
       - language containers
       - speech containers
       - vision containers
   - Consume Azure AI services from a container: no need to provide subcription key, but has to implement your own authentication solution and network security restrictions.

## Create computer vision solutions with Azure AI Vision
 - Analyze images
   - Provision an Azure AI Vision resource
     - Description and tag generation - determining an appropriate caption for an image, and identifying relevant "tags" that can be used as keywords to indicate its subject.
     - Object detection - detecting the presence and location of specific objects within the image.
     - People detection - detecting the presence, location, and features of people in the image.
     - Image metadata, color, and type analysis - determining the format and size of an image, its dominant color palette, and whether it contains clip art.
     - Category identification - identifying an appropriate categorization for the image, and if it contains any known landmarks.
     - Background removal - detecting the background in an image and output the image with the background transparent or a greyscale alpha matte image.
     - Moderation rating - determine if the image includes any adult or violent content.
     - Optical character recognition - reading text in the image.
     - Smart thumbnail generation - identifying the main region of interest in the image to create a smaller "thumbnail" version.
   - Analyze an image:
     - sdk: create an `imageAnalysisClient` with `endpoint` and `key`; use `analyze` method with property `visual_features` assigned with `[VirsualFeatures]` (`VisualFeature enum`: TAGS, OBJECTS, CAPTION, DENSE_CAPTIONS,PEOPLE, SMART_CROPS,READ)
   - Generate a smart-cropped thumbnail and remove background:
     - thumbnail: specify `dimensions` and `region of interest` of the image
     - remove background: `alpha matte` of the foreground subject
 - Image classification with custom Azure AI Vision models
   - Create a custom Azure AI Vision classification model: (the types of custom models include `image classification`,`object detection`, and `product recognition`:more accuracy than `object detection`) first create an azure ai services resource or azure ai vision resource, then create a custom project.
     - Components of a custom Vision project: a `dataset` is the collection of images which is stored in an azure blob storage container, and a `COCO` file is the file that defines the labal info about those images. And the steps to take to train models:
       - create blob storage container and upload images
       - create dataset and connect it to the blob storage container, also define the type of project
       - label your data in azure ml data labeling project, which creates the COCO file in the blob storage container (each category need 3-5 images)
       - connect COCO file to the dataset
       - train custom model on dataset and labels
       - verify performance and iterate it if need
     - COCO files(json file):
       - images
       - annotations
       - categories
     - Creating your dataset
   - Understand image classification: classify the images based on their contents. models can be trained for multi-class or multi-label.
   - Understand object detection:
     - the class label of each object detected in the image
     - the location of each object within the image(bounding box)
   - Train an image classifier in Vision Studio
 - Detect, analyze, and recognize faces
   - Identify options for face detection, analysis, and identification.
     - Azure ai vision service: detect people and bounding box
     - face service: more comprehensive solution: face detection, facial feature analysis, face comparison and verification, facial recognition
   - Understand considerations for face analysis
     - data privacy and security
     - transparency
     - faireness and inclusiveness
   - Detect faces with the Computer Vision service: call `analyze iamge` function(sdk) or rest api requests, specifying `people` as `visual_feature`. Return the bounding box and basic attributes
   - Understand capabilities of the Face service
     - Face detection - for each detected face, the results include an ID that identifies the face and the bounding box coordinates indicating its location in the image.
     - Face attribute analysis - you can return a wide range of facial attributes, including:
       - Head pose (pitch, roll, and yaw orientation in 3D space)
       - Glasses (NoGlasses, ReadingGlasses, Sunglasses, or Swimming Goggles)
       - Blur (low, medium, or high)
       - Exposure (underExposure, goodExposure, or overExposure)
       - Noise (visual noise in the image)
       - Occlusion (objects obscuring the face)
       - Accessories (glasses, headwear, mask)
       - QualityForRecognition (low, medium, or high)
     - Facial landmark location - coordinates for key landmarks in relation to facial features (for example, eye corners, pupils, tip of nose, and so on)
     - Face comparison - you can compare faces across multiple images for similarity (to find individuals with similar facial features) and verification (to determine that a face in one image is the same person as a face in another image)
     - Facial recognition - you can train a model with a collection of faces belonging to specific individuals, and use the model to identify those people in new images.
     - Facial liveness - liveness can be used to determine if the input video is a real stream or a fake to prevent bad intentioned individuals from spoofing the recognition system.
   - Compare and match detected faces: when a face is detected, a unique id will be assigned to it and retained in the service resource for 24 hrs, and this id has no indication of this individual's identity other than their facial features, which can be used for facial comparison.
   - Implement facial recognition: (this feature is related to personal information, thus needs to apply for use)
     - create a `Person Group` that defines the sets of people you wanna identity
     - add `Person` to the `Person Group` for each person
     - add detected faces from multiple images to each `person`, the id for the person will persist (`persisted faces`)
     - lastly, train the model
     - the model can be used to identity individuals, verify identity of a detected face, analyze new images to find faces that are similar to a known one
 - Read Text in images and documents with the Azure AI Vision Service
   - options for reading texts:
     - image analysis OCR: used for general unstructured documents with small amount of text, or images that contain text; synchronous api call; has functionalities for analyzing images past exrtacting text
     - document intelligence: used for smail to large volumes of text from images and pdf documents; uses context and structure of the document to improve accuracy; asynchronous api call
   - Read text from images using OCR: use `imageAnalysis` function with `read` `visual_feature`. the return json results: `blocks`, `lines`,`text`/`words`
   - Use the Azure AI Vision service Image Analysis with SDKs and the REST API
   - Develop an application that can read printed and handwritten text
 - Analyze video
   - Describe Azure Video Indexer capabilities
     - Facial recognition - detecting the presence of individual people in the image. This requires Limited Access approval.
     - Optical character recognition - reading text in the video.
     - Speech transcription - creating a text transcript of spoken dialog in the video.
     - Topics - identification of key topics discussed in the video.
     - Sentiment - analysis of how positive or negative segments within the video are.
     - Labels - label tags that identify key objects or themes throughout the video.
     - Content moderation - detection of adult or violent themes in the video.
     - Scene segmentation - a breakdown of the video into its constituent scenes.
   - Extract custom insights: Azure Video Indexer includes predefined models that can recognize well-known celebrities, do OCR, and transcribe spoken phrases into text. You can extend the recognition capabilities of Video Analyzer by creating custom models for:
     - `people`: add images of the faces appear in the videos, and train the model to make video indexer recognize them
     - `language`: train a custom model to detect and transcribe specific terminology
     - `brands`; train a custom model to recognize names of brands, products,etc
   - Use Azure Video Indexer widgets and APIs: `Azure Video Indexer widgets` can be embedded into custom html interfaces, sharing insights from specific videos with others wihout giving people full access to your account in the azure video indexer portal. Or using `azure video indexer api` to create projects, retrive insights or modify models.
     - using `Azure resource manager` template to create azure ai video indexer resource 

## Develop natural language processing solutions with Azure AI Services
 - Analyze text with Azure AI Language
   - Detect language from text:
     - confidence score: 0 - 1
     - document size < 5120 characters
     - each collection limit: 1000, each item has a unique `id` and `text` (can also pass `countryHint` to improve prediction performance)
     - mixed language content returns the largest representation with a lower positive rateing
     - for the content where there is ambguity, response will be `unknown` with confidence score `0`
   - Analyze text sentiment: evaluate how positive and nagative the content is.
     - response contains overall sentiment and each sentence sentiment
     - sentence sentiment: `positive`, `negative`,`neutral`
     - all `neutral` sentences --> `neutral` document
     - only `positve` and `neutral` sentences --> `positive` document
     - only `negative` and `neutral` sentences --> `negative` document
     - `positive` and `negative` sentences --> `mixed` document
   - Extract key phrases, entities, and linked entities
     - extract key phrases: works best with larger documents(max: 5120 characters), can also pass multiple documents
     - entities: `person`, `location`,`datetime`,`organization`,`address`,`email`,`url`
     - linked entities: to disambiguate entities of the same name by referencing an article in a knowledge base.
 - Create question answering solutions with Azure AI Language
   - Understand question answering and how it compares to language understanding.
     - understand question answering: define a `knowledge base` of question and answer pairs that can be queried using natural language input. it can be published and  consumed by client apps, or bots. `knowledge base` is created from existing sources: web sites with `FAQ` documentation; files with structured text, like brochures or user guides; built-in chit char question and answer pairs having conversational exchanges.
     - question answering vs language undestanding
   - Create, test, publish, and consume a knowledge base.
     - create: `ai language` service, enable `question answering` feature; create `ai search` resource to host the index of the knowledge base; go to `language studio` to create `custom question answering` project; then add data sources to knowledge base; lastly edit question and answer pairs
     - test:
     - publish:
     - consume: through REST Api (`question`,`top`,`scoreThreshold`,`strictFilters`)
   - Implement multi-turn conversation and active learning:
     - multi-turn: ask clients follow-up questions to elicit more information from clients before presenting a definitive answer.
     - active learning: improve knowledge base by `active learning`(different questions with the same meanning, enabled by default) and defining `synonyms`(words with the same meaning)
   - Create a question answering bot to interact with using natural language.
 - Build a conversational language understanding model
   - Azure ai language service: pre-configured features and learned features
     - pre-configured features: `Summarization`,`Named entity recognition`,`Personally identifiable information (PII) detection`,`Key phrase extraction`,`Sentiment analysis`,`Language detection`.
     - learned features: `Conversational language understanding (CLU)`,`Custom named entity recognition`,`Custom text classification`,`Question answering`
   - Provision Azure resources for Azure AI Language resource
   - Define intents, utterances, and entities
     - utterances: phrases that a user might enter when interacting with an app that uses your language model.
     - intent: a task or action that a user wanna perform( the meaning of the utterance). Note: every model has a `None` intent that should be explicitly identify utterances with no action required or that falls outside of the scope of the domain of this model.
     - entities: used to add specific context to intents
       - learned entities
       - list entities
       - prebuilt entities
   - Use patterns to differentiate similar utterances: similar but with different meanings of utterances.
   - Use pre-built entity components
   - Train, test, publish, and review an Azure AI Language model
     - Train a model to learn intents and entities from sample utterances.
     - Test the model interactively or using a testing dataset with known labels
     - Deploy a trained model to a public endpoint so client apps can use it
     - Review predictions and iterate on utterances to train your model 
 - Create a custom text classification solution
   - Understand types of classification projects
     - single label classification
     - multiple label classification
     - labeling data
     - evaluating and improving your model: false positive, false negative
       - `recall`: Of all the actual labels, how many were identified; the ratio of true positives to all that was labeled.
       - `precision`: How many of the predicted labels are correct; the ratio of true positives to all identified positives.
       - `f1 scrore`: A function of recall and precision, intended to provide a single score to maximize for a balance of each component
   - Build a custom text classification project
     - define labels
     - tag data
     - train model
       - training set(80% recommended) and testing set
       - automatic split( great for large dataset) and manual spliy(best for smaller datasets)
     - view model
     - improve model(iterate)
     - deploy model
     - consume model
   - Tag data, train, and deploy a model
   - Submit classification tasks from your own app
 - Custom named entity recognition
   - Understand tagging entities in extraction projects
     - considerations for data selection and refining entities: high quality data(diversity, distribution, accuracy)
     - project limits:
       - training: > 10 files and < 100,000 files
       - 10 deployments per project
       - apis:
         - authoring: this api creates a project, trains and deploys your model. limited to 10 post and 100 get per min
         - analyze: this api extracts entities; requires a task and retrieves the results. limited to 20 get or post.
       - projects: 1 storage account per project; 500 projects per resource; 50 trained model per project
       - entities: 500 characters per entity, up to 200 entity types
     - label your data: (`consistency`,`percision`,`completeness`)
   - Understand how to build entity recognition projects
 - Translate text with Azure AI Translator service
   - Provision a Translator resource
   - Understand language detection, translation, and transliteration
   - Specify translation options
     - word alignment
     - sentence length
     - profanity filtering: `NoAction`,`Deleted`,`Marked`
   - Define custom translations
 - Create speech-enabled apps with Azure AI services
   - Provision an Azure resource for the Azure AI Speech service
   - Use the Azure AI Speech to text API to implement speech recognition
     - speech to text api & speech to text short audio api
     - `using the azure ai speech sdk`:
       - `speechConfig`: localtion and key to connect to azure ai speech resource
       - `audioConfig`: define the input source for the audio to be transcribed.
       - `speechConfig` & `audioConfig`--> `speechRecognizer` (proxy client) for `speech to text api`
       - call `recognizeOnceAsync` to asynchronously transcribe a single spoken utterance.
       - the result `speechRecognitionResult`: `duration`,`offsetInticks`,`properties`,`reason`(nomatch,cancelled,recognizedspeech),`resultid`,`text`
   - Use the Text to speech API to implement speech synthesis
     - text to speech api & batch synthesis api
     - `using the azure ai speeck sdk`:
       - `speechConfig`: localtion and key to connect to azure ai speech resource
       - `audioConfig`: define the output device for the speech to be synthesized.
       - `speechConfig` & `audioConfig`--> `speechSynthesizer` (proxy client) for `text to speech api`
       - call `speakTextAsync` to asynchronously convert text to a single spoken audio.
       - the result `speechSynthesisResult`: `audioData`,`properties`,`reason`(cancelled,synthesizingAudioCompleted),`resultid`
   - Configure audio format and voices:
     - audio format(setspeechSynthesisOutputFormat): `audio file type`,`smaple-rate`,`bit-depth`
     - voices(speechSynthesisVoiceName): `standard voices`,`neural voices`
   - Use Speech Synthesis Markup Language (SSML): xml-based syntax for describing characteristics of the speech you wanna generate:
     - specify a speaking style
     - insert pauses or silence
     - specify phonemes
     - adjest the prosody of the voice
     - use common 'say-as' rules
     - insert recorded speech or audio (simulate background noise)
 - Translate speech with the Azure AI Speech service
   - Provision Azure resources for speech translation.
   - Generate text translation from speech.
     - `speechTranslationConfig`: location and key
     - `SpeechTranslationConfig`: input language and the target language
     - `AudioConfig`: define the input source for the audio
     - `SpeechTranslationConfig` & `AudioConfig` --> `TranslationRecognizer` proxy client
     - use `RecognizeOnceAsync` to asynchronously translate a single spoken utterance
     - result `SpeechRecognitionResult`: `duration`,`offsetInticks`,`properties`,`reason`,`resultid`,`text`,`translations`
   - Synthesize spoken translations
     - event-based synthesis: `TranslationRecognizer` has a `synthesizing` event, then create an event handler and use`getAudio` method of the `result` parameter to retrieve the byte stream of translated audio.
     - manual synthesis: iterate the `translations` dictionary and use `speechSynthesizer` to synthesize an audio stream for each language

## Implement knowledge mining with Azure AI Search
 - Create an Azure AI Search solution
   - Create an Azure AI Search solution
     - manage capacity: `free`,`basic`,`standard`(s,s2,s3),`storage optimized`(l1,l2)
     - replicas and partitions: `replica`: instance of the search service, `partition`: used to divide an index into multiple storage locations. Together, it is called `search units`(replicas * partitions)
     - understand search components:
       - data source: unstructured files in azure blob storage containers; azure sql db tables; documents in cosmosDB; or directly push JSON data into `index`
       - skillset: an enrichment pipeline used by `indexer` where each step enhances the data with insights obtained by a specific ai skill. such as `the language`, `key phrases`,`sentiment score`,`entities`,`image analysis`, or some custom skills
       - indexer: an engine that drives overall indexing process. it takes the outputs from skills of the skillset, along with the original data from the data source, and maps them to the fields in the index.
       - index: a searchable result of the indexing process. It consists of a collection of JSON documents with fields that contains the values extracted from indexing. the attributes:
         - `key`: a unique key for the field
         - `searchable`: full-text search support: search a text in a text data
         - `filterable`: can be used in filter expressions
         - `sortable`: support sorting
         - `facetable`: fields that can be used for `facets`
         - `retrievable`: by default, all fields are retrievable
     - understand indexing process: start by creating a `document`(a JSON file), you can extract image data and put them into `normalized_images` property. also each skill will add fields into this `document`. or create `merged_content` field using a skill which would use other fields as inputs
   - Develop a search application
     - search an index
       - full text search: `simple`, `full`
       - search parameters: `search`,`queryType`,`searchFields`,`select`,`searchMode`
       - query processing:
         - query parsing
         - lexical analysis
         - document retrieval
         - scoring
     - apply filtering and sorting
       - filtering results, filtering with facets, sorting results
     - enhance the index
       - search-as-you-type: add `suggester` to the index: `suggestion` or `autocomplete`
       - custom scoring and result boosting: define `scoring profile`, can be used on single search or on index definition
       - synonyms: define `synonym maps` to link related items
 - Create a custom skill for Azure AI Search
   - Implement a custom skill for Azure AI Search: input and output schemas that should be followed when creating custom skill
   - Integrate a custom skill into an Azure AI Search skillset: using `custom.webApiSkill` skill type
 - Create a knowledge store with Azure AI Search
   - knowledge store: `export index as JSON files`,`normalize the index into relational schema of tables`, `images extracted from the indexing process`
   - projection: each skill will iteratively build the `document`, and you can persist some or all of the fields as `projections`. Also using `shaper` skill to create new fields with simpler structure for the fields you wanna map to projections
   - Create a knowledge store from an Azure AI Search pipeline
   - View data in projections in a knowledge store
 - Enrich your data with Azure AI Language
   - Use Azure AI Language to enrich Azure AI Search indexes.
     - azure ai language features: `classify text`,`understand questions and conversational language`,`extract information`,`summerize text`,`translate text`
   - Enrich an AI Search index with custom classes
     - store search data for ai language studio and ai search indexers
     - create language studio project
     - train and test your model
     - create search index on stored documents
     - create a function to use the trained model
     - update search solution, index,indexer and custom skiller
 - Implement advanced search features in Azure AI Search
   - Improve the ranking of a document with term boosting: can enable full-text search by adding`&queryType=full`, and unlock a couple of features: `fuzzy search`, `term proximity search`, `term boosting`,`regular expression search`, etc
   - Improve the relevance of results by adding scoring profiles: `weights` or `function`
   - Improve an index with analyzers and tokenized terms
     - analyzer: lucene analyzer(default), or other language analyzers or specialized analyzers
     - custom analyzers:
       - character filters: `html strp`,`mapping`,`pattern_replace`
       - tokenizers: break words down into tokens and root forms
       - token filters: further process tokens
       - can use `analyzer` field on indexing or searching
   - Enhance an index to include multiple languages:
     - add fields that need a translation
     - add translation skill to the skillset
     - update indexer to map the translated text to the correct fields in the index
   - Improve search experience by ordering results by distance from a given reference point
     - geo-spatial fucntions: `geo.distance`,`geo.intersects`
 - Build an Azure Machine Learning custom skill for Azure AI Search
   - Understand how to use a custom Azure Machine Learning skillset
     - `AmlSkill` custom skill type
   - Use Azure Machine Learning to enrich Azure AI Search indexes.
 - Search data outside the Azure platform in Azure AI Search using Azure Data Factory
   - Use Azure Data Factory to copy data into an Azure AI Search Index
     - create an ADF pipeline to push data into a search index
     - limits: only support certain data types
   - Use the Azure AI Search push API to add to an index from any external data source
     - exponential backoff strategy with batching operation
     - outsider function using multi-threading
 - Maintain an Azure AI Search solution
   - manage security of ai search solution
     - data encryption: encrypted at rest and in-transit
     - secure inbound traffic: azure gateway, and firewall for public endpoint
     - secure outbound traffic: indexer to data sources (key-based authentication, database login, or microsoft entra login), and using firewall to restrict access to azure services.
     - secure data at the document level: add a new security field(filterable) to every document that contains the user or group IDs that can access it. then when querying, add `search.in` to filter to certain group of users
   - optimize performance of ai search solution
     - manage the current search performance
     - check if your search service is throttled
     - check the performance of individual queries
     - optimize your index size and schema
     - improve the performance of your queries
     - use the best service tier for your search needs
   - manage costs of ai search solution
     - Estimate your search solutions baseline costs
     - Understand the billing model
     - Tips to reduce the cost of your search solution
       - as few region as possible, all resources should be in the same region
       - for predictable patterns of indexing new data, considering scaling up inside your search tier, then scale back down for your regular querying
       - to keep search requests and responses in the azure datacenter boundary, use azure web app front-end as your search app
       - enable enrichment caching if using ai enrichment on blob storage
     - using budgets and alerts
   - improve reliability of ai search solution
     - Make your search solution highly available
     - Distribute your search solution globally
     - Back up options for your search indexes: there is official solution to backup and restore mechanism for azure ai search. you have to backup yourself
   - monitor ai search solution
     - Monitor Azure AI Search in Azure Monitor: access `log analytics`(`azureActivity`,`azureDiagnostics`,`azureMetrics`)
     - Use metrics to see diagnostic data visually
     - Write Kusto queries against your search solutions logs
     - Create alerts to be notified about common search solution issues: `search latency`,`throttle search percentage`,`delete search service`,`stop search service`
   - debug search issues using azure portal
     - Explore how to use the Debug Session tool in Azure AI Search
     - Debug a skillset with Debug Sessions
       - create a debug session
       - explore and edit a skill
       - validate the field mappings
 - Perform search re-ranking with semantic ranking in Azure AI Search
   - Describe semantic ranking: aims to improve the ranking of search results.
     - BM25 ranking function, but it does not place any relevance on the semantics of the query, which need language understanding.
     - semantic ranking has two functions; it uses BM25 and language understand models to calculates a new relevance score
     - semantic captions and answers: Semantic captions extract summary sentences from the document verbatim and highlight the most relevant text in the summary sentences. Semantic answers is an optional additional feature of semantic ranking that provides answers to questions.
     - How semantic ranking works: using BM25 to get top 50 results, then results will be split into multiple fields which will be converted into text strings and then be passed to machine reading comprehension models to find the phrases and sentences that best match the query.
     - advantages:
       - semantic ranking makes results more closely matched the original query
       - can generate semantic captions and answers
     - limits: no other additional info that weren't returned by BM25; only the top 50 results will be considered
     - pricing: first 1000 queries free per month
   - Set up semantic ranking: enabled at service level, working for all indexes, and then configure it on a per-index basis
   - Perform semantic ranking on an index
 - Perform vector search and retrieval in Azure AI Search
   - Describe vector search: Vector search is a new capability available in AI Search used to index, store and retrieve vector embedding from a search index. You can use it to power applications implementing the Retrieval Augmented Generation (RAG) architecture, similarity and multi-modal searches or recommendation engines. A vector query can be used to match criteria across different types of source data by providing a mathematical representation of the content generated by machine learning models. This eliminates the limitations of text based searches returning relevant results by using the intent of the query.
     - when to use vector search
       - use openai or other open source model to encode text and use queries encoded as vector to retrieve documents
       - do a similarity search across encoded images, text, video and audio or mix
       - build a hybrid search using vector and searchable text, vector search is implemented at field level, the results will be merged into a single response.
       - creaet a vector database
       - add filters to text and numeric fields and include this in the query to reduce the data that vector search needs to process
   - prepare search:
     - check index has vector fields: run an empty query to see if there is a `vector` field with a number array. or look for a field named `vectorSearch` with a type `Collection(edm.single)`, which has an algorithm configuration and an attribute of 'dimension'.
     - convert a query input to a vector (an embedding model will convert query to a vector)
   - Describe embeddings: An embedding is type of data representation that is used by machine learning models. an embedding as an array of numbers, and the numerical distance between two embeddings represents their semantic similarity.
     - embedding models: different models for different types of search `similarity search`,`text search`,`code search`
     - embedding space: In this embedding space, similar items are located close together, and dissimilar items are located farther apart.
   - Run vector search queries using the REST API

## Develop solutions with Azure AI Document Intelligence
 - Plan an Azure AI Document Intelligence solution
   - Describe the components of an Azure AI Document Intelligence solution.
     - prebuilt models for general document analysis and other common forms or documents
     - azure ai document intelligence is a high-level ai service built on lower level ai services, including ai vision.
     - document intelligence is more sophisticated solution for document analysis
     - azure ai document intelligence tool
   - Create and connect to Azure AI Document Intelligence resources in Azure.
   - Choose whether to use a prebuilt, custom, or composed model.
     - prebuilt models: general document analysis models(read, general document, layout)
     - specific document type models: `invoice`,`receipt`,`w-2`,`id document`,`business card`, `health insurance card`
     - custom models: `custom template models`,`custom neural models`
     - composed models: consists of multiple custom models
 - Use prebuilt Document intelligence models
   - Identify business problems that you can solve by using prebuilt models in Forms Analyzer.
     - features: `text extraction`,`key-value pairs`,`entities`,`selection marks`,`tables`,`fields`
     - input requirements:
       - type: jpeg, png, bmp, tiff, pdf, or microsoft office files
       - < 500mb standard tier, < 4mb for free tier
       - dimentions between 50*50 -- 10,000 * 10,000
       - pdf dimention < 17 * 17 inches or A3 paper size
       - total size of the training data set < 500 pages
       - pdf no password protected
       - pdf and tiff: first 2000 pages for standard tier, first 2 pages for free tier
   - Analyze forms by using the General Document, Read, and Layout models:
     - for document with unpredictable structure, can use read, general document or layout
   - Analyze forms by using financial, ID, and tax prebuilt models.
 - Extract data from forms with Azure Document intelligence
   - Identify how Document intelligence's layout service, prebuilt models, and custom models can automate processes.
   - Use Document intelligence's capabilities with SDKs, REST API, and Document Intelligence Studio.
   - Develop and test custom models.: (ocr.json, labels.json, fields.json)
 - Create a composed Document intelligence model
   - Describe business problems that you would use custom models and composed models to solve.
     - **Custom neural models can't be composed with custom template models
**
   - Train a custom model to obtain data from forms with unusual structures.
   - Create a composed model that can analyze forms in multiple formats.
     - via document intelligence studio
     - via code
 - Build a Document intelligence custom skill for Azure AI search
   - Describe how a custom skill can enrich content passed through an Azure AI Search pipeline.
   - Build a custom skill that calls an Azure Forms Analyzer solution to obtain data from forms.


## Develop Generative AI solutions with Azure OpenAI Service
 - Get started with Azure OpenAI Service
   - Create an Azure OpenAI Service resource and understand types of Azure OpenAI base models(gpt-4, gpt-3.5, embeddings models, dall-e models).
   - Use the Azure OpenAI Studio, console, or REST API to deploy a base model and test it in the Studio's playgrounds.
     - completion playground
       - temperature: low --> high randomness
       - max lenght(tokens)
       - stop sequences: make responses stop at a desired point, such as the end of sentence or list
       - top probabilities(top P): low --> high randomness (do not configure it along with temperature)
       - frequency penalty: reduce the likelihood of repeating the same text in a response
       - presence penalty: reduce the likelihood of any token that has appeared in the text at all so far, which increase the likelihood of introducing new topics in a response.
       - pre-response text: can help prepare the model for a response
       - post-response text: can encourage further user input
     - chat playground: zero-shot or few-shot
       - max response: api supports a maximum of 4000 tokens per prompt
       - top P: the same as that of completion playground
       - past messages included: number of past messages to include in each new api request to give the model context for new user queries
   - Generate completions to prompts and begin to manage model parameters.
     - prompts: a text portion of a request sent to the deployed model endpoint
     - completions: a response comes back in form of text, code, or other formats
     - prompts types:
       - classifying content
       - generating new content
       - holding a conversation
       - transformation(translation and symbol conversion)
       - summarizing content
       - picking up where you left off
       - giving factual responses
     - completion quality:
       - the way a prompt is engineered
       - model parameters
       - data the model is trained on
 - Build natural language solutions with Azure OpenAI Service
   - Integrate Azure OpenAI into your application
     - create an azure openai resource
     - choose a model to deploy
     - authentication and specification of deployed model
     - prompt engineering
   - Differentiate between different endpoints available to your application
     - completion
     - chatCompletion
     - embeddings
   - Generate completions to prompts using the REST API and language specific SDKs
 - Apply prompt engineering with Azure OpenAI Service
   - Understand the concept of prompt engineering and its role in optimizing Azure OpenAI models' performance.
     - prompt engineering: the process of designing and optimizing prompts to better utilize AI models. (can also design prompts to understand how models reach the response or how models make decisions)
     - parameters: `top P`, `temperature` (do not configure both at the same time)
   - Know how to design and optimize prompts to better utilize AI models.
     - provide clear instructions: recency bias can affects models. placing instructions(or most recent messages) at the end of the prompt will make models generate better responses
     - use section markers: `---` or `###` to contain content to help models differenciate between instructions and contents
     - primary, supporting, and grounding content: `primary content`: the subject of the query. `supporting content`: the content that may alter the response, not focus on the subject of the prompt. `grounding content`: allows the model to provide more reliable answers by providing content for the model to draw answer from. 
     - cues: used with instructions, particularly useful when generating code
   - Include clear instructions, request output composition, and use contextual content to improve the quality of the model's responses.
     - request output composition
     - system message: at the beginning of the propmts
     - conversation history (chatgpt includes history automatically)
     - few shot learning
     - break down a complex task
     - chain of thought--> a method to help break down your task effectively is to ask the model to explain its chain of thought.
 - Generate code with Azure OpenAI Service
   - Use natural language prompts to write code
     - write functions
     - change coding language
     - understand unknown code
   - Build unit tests and understand complex code with AI models
     - complete partial code
     - write unit tests
     - add comments and generate documentation
   - Generate comments and documentation for existing code
   - fix bugs and improve your code
   - refactor inefficient code
 - Generate images with Azure OpenAI Service
   - Describe the capabilities of DALL-E in the Azure openAI service: a neural network based model that can generate graphical data from natural language input.
   - Use the DALL-E playground in Azure OpenAI Studio: settings: `resolution`,`image style`,`image quality`
   - Use the Azure OpenAI REST interface to integrate DALL-E image generation into your apps: (`prompt`,`n`,`size`) for initial request, then the response has a `operation-location` header with a url to call to poll the generated images
 - Implement Retrieval Augmented Generation (RAG) with Azure OpenAI Service
   - understand RAG with azure openai service: models can reference both the specific data provided and its pretrained knowledge to provide more effective responses.
     - receive the prompt
     - analyze the prompt (relevant content and intent)
     - query the search index with the content and intent
     - insert search result into azure openai prompt, along with system message and user prompt
     - send the entire prompt to azure openai
     - return the response
     - `by default, openai encourage model to use your data, but it can be unseleted, and when unselected it, model will only use its pre-trained knowledge`
     - `fine-tuning vs RAG`: `fine-tuning`: train an existing model with additional data, which is more costly and time-consuming. `RAG`: remove the requirement of training a custom model and simplify the interaction with the model, the ai search service will find useful information and add it to the prompt as grounding data, and then azure openai will generate responses based on that information.
   - add your own data and chat with your model using your own data
     - limits: system message < 200 tokens; response < 1500 tokens
 - Fundamentals of Responsible Generative AI
   - Describe an overall process for responsible generative AI solution development
     - Identify potential harms that are relevant to your planned solution.
     - Measure the presence of these harms in the outputs generated by your solution.
     - Mitigate the harms at multiple layers in your solution to minimize their presence and impact, and ensure transparent communication about potential risks to users.
     - Operate the solution responsibly by defining and following a deployment and operational readiness plan.
   - Identify and prioritize potential harms relevant to a generative AI solution
     - identify potential harms
     - prioritize identified harms
     - test and verify the prioritized harms
     - document and share the verified harms
   - Measure the presence of harms in a generative AI solution
     - Prepare a diverse selection of input prompts that are likely to result in each potential harm that you have documented for the system.
     - Submit the prompts to the system and retrieve the generated output.
     - Apply pre-defined criteria to evaluate the output and categorize it according to the level of potential harm it contains.
     - manual and automatic testing: complement to each other
   - Mitigate harms in a generative AI solution
     - model layer: select a proper model or fine-tune a base model
     - safety system layer: azure openai service includes `content filter`, and azure network monitoring and alerts and firewall 
     - metaprompt and grounding layer: focus on construction of prompts that are submitted to the model. `metaprompts or system inputs to define the behaviors of the model`,`add grounding data to prompts`, `use RAG to retrieve contextual data from trusted data sources and include it in prompts`
     - user experience layer: application level to restrict user inputs, validate inputs and outputs; documentation and other descriptions of ai solution should be transparent about capabilities and limitations as well as any potential harms of the system.
   - Prepare to deploy and operate a generative AI solution responsibly
     - complete prerelease reviews:
       - legal
       - privacy
       - security
       - accessibility
     - release and operate the solution
       - devise a phased delivery plan, to release to a small group of users, listen to the feedback before releasing to a wider audience
       - create an incident response plan to estimate the reaction time to any unexpected incidents
       - create a rollback plan in case something bad happened to the solution
       - create a capability to immediately block harmful responses when discovered
       - create a capability to block certain users, apps, or ip addresses when misuse
       - create a way for users to provide feedback and report issues
       - track telemetry data that allows you to determine user satisfaction and identify functional gaps or usability challenges (must follow privacy laws and policies)
      

## Assessments
 - 1st test:
   - A virtual machine in Azure belongs to a virtual network, if we wanna our app which is deployed to a virtual machine to access Azure ai services, then at the firewall level, we can grant access to the virtual network to the specific ai service endpoint.
   - the prerequisites for diagnostic logging are `a log analytics workplace`, `a storage account`
   - `mismatch` error: the api key is not for the correct endpoint of certain kina of azure ai services resource
   - `invalid` error: api key is not for the correct regions.
   - Model_2 Improves accuracy on small, side-view, and blurry faces. (`faceIdTimeToLive` is used for face id caching)
   - for document intelligence, the S0 tier has limit of 500mb and 2000 pages. But, if documents are password-protected, then the process will fail.
   - You need to upload the video, get the video index, and get the thumbnail for each keyframe. Three API calls need to be done to extract keyframes and store them in a disk.
   - custom brands models supports brand detection from speech and visuals.
   - Slate detection is used for clapper boards and digital patterns with color bars, and the custom Language model is used to add words that are not in the model.
   - for ai video indexer, a sentence with special characters will be discarded
   - When training the model, you should avoid repeating an identical sentence multiple times, as it may create bias against the rest of the input. You should avoid including uncommon symbols (~, # @ % &), as they will be discarded. The sentences in which they appear will also be discarded. You should also avoid putting inputs that are too large, such as hundreds of thousands of sentences, because doing so will dilute the effect of boosting.
   - for azure language service, `opinionMining=true` will add aspect-based sentiment analysis, which in turn will make the sentiment more granular so that positive and negative in a single sentence can be returned.
   - key phrase extraction is the feature of azure ai language, and while summarization is a feature that extracts sentences that collectively represent the most important or relevant information within the original content.
   - speech service errors:
     - `Substitution errors` are due to the model needing more training on custom product names and people names in speech-to-text feature of azure ai services.
     - `deletion errors`: Overlapping speakers define.
     - `insertion errors`: People talking in the background are detected.
   - speaker recognition: determine who is speaking, verify and identify
   - speaker verification: text-dependent or text-independent
   - speaker identification: determine an unknown speaker
   - Azure Storage is the only storage provider that can be used by default for batch transcription.
   - The `List entity` is made up of a list of phrases that will guide the engine on how to match the text. When an entity has an ID of type List and is in Strict mode, the engine will only match if the text in the slot appears in the list.
   - for training custom translator, when using bilingual training documents, `be liberal`, any in-domain human translation is better than machine translation (increase bilingual evaluation understudy: BLEU score)
   - BLEU scores: 0 - 100, (40 -60 means a high-quality translation)
   - If the language of the content in the source document is known, it is recommended to specify the source language in the request to get a better translation.
   - ai translator features: `detect language`,`transliterate`,`dictionary lookup`,`dictionary example`
   - Orchestration workflow projects do not support the multilingual option, so you need to create a separate workflow project for each language, which is an azure ai language service feature.
   - Precision measures how precise/accurate a model is.
   - Recall measures the model's ability to predict actual positive classes.
   - F1 score is a function of precision and recall.
   - Active learning is turned on by default. You can use active learning for this instead of manually logging the questions and adding them.
   - Active learning suggestions are not in real time. There is an approximate delay of 30 minutes before suggestions show on this pane. This delay balances the high cost involved in real-time updates to the index and service performance.
   - The throughput, the size, and the number of knowledge bases affect the pricing tier
   - ai search projection types:
     - tables: data that is best represented as rows and columns, or whenever you need granular representations of your data.
     - files: when you need to save normalized, binary image files.
     - objects: when you need the full JSON representation of your data and enrichments in one JSON document.
   - ai search queryType:
     - simple: query input as-is
     - full: wildcard, fuzzy, regex, field-scoped queries...
   - Document cracking is the process of opening files and extracting content. It is the first stage of the indexing process. (document cracking, field mappings, skillset execution, output fields mappings, push into index)
   - `Microsoft.Skills.Util.DocumentExtractionSkill` is the built-in skill used to extract content from a file within the enrichment pipeline.
   - `name`, `description`, and `skills` are required for defining `skillset`
   - document intelligence supports pdf, and microsoft office files
   - The system message is included at the beginning of the prompt and is used to prime the model with context, instructions, or other information relevant to the use case. You can use the system message to describe the assistants personality, define what the model should and should not answer, and define the format of model responses.
   - When the retirement date is reached, the model will upgrade to the default version automatically at the time of retirement.
   - for DAll-E, The result from the initial request does not immediately return the results of the image generation process. Instead, the response includes an `operation-location` header with a URL for a callback service that your application code can poll until the results of the image generation are ready. The `result` element includes a collection of `url` elements, each of which references a PNG image file generated from the prompt.
   - to make http requests to azure openai dall-e model api, you need to ensure that http headers includes `api version`,`name of azure openai service resource`,`name of dall-e model deployment`
   - when making http requests to azure openai dall-e model API, the body is the prompt
   - Azure OpenAI on your data enables developers to use supported AI chat models that can reference specific sources of information to ground the response. Adding this information allows the model to reference both the specific data provided and its pretrained knowledge to provide more effective responses.
   - The Strictness parameter sets the threshold to categorize documents as relevant to your queries. Raising the Strictness parameter value means a higher threshold for relevance and filters out more less-relevant documents for responses.
   - for prompting: `Be Specific` means to leave as little to interpretation as possible. `Be Descriptive` means to use analogies. `Order Matters` means that the order in which you present information to the model can affect the output. Therefore, those three are valid best practices.








